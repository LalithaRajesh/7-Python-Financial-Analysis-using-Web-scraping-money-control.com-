{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#means it looks at the Python module request (found within the urllib library) and imports only the function urlopen.\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urllib or urllib2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve used the urllib2 library in Python 2.x, you might have noticed that things have changed somewhat between urllib2 and urllib. In Python 3.x, urllib2 was renamed urllib and was split into several submodules: urllib.request, urllib.parse, and urllib.error. Although function names mostly remain the same, you might want to note which functions have moved to submodules when using the new urllib.\n",
    "\n",
    "urllib is a standard Python library (meaning you don’t have to install anything extra to run this example) and contains functions for requesting data across the web, handling cookies, and even changing metadata such as headers and your user agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Beautiful Soup, so rich and green,\n",
    "Waiting in a hot tureen!\n",
    "Who for such dainties would not stoop?\n",
    "Soup of the evening, beautiful Soup!”\n",
    "\n",
    "The BeautifulSoup library was named after a Lewis Carroll poem of the same name in Alice’s Adventures in Wonderland. In the story, this poem is sung by a character called the Mock Turtle (itself a pun on the popular Victorian dish Mock Turtle Soup made not of turtle but of cow).Like its Wonderland namesake, BeautifulSoup tries to make sense of the nonsensical;\n",
    "it helps format and organize the messy web by fixing bad HTML and presenting us with easily-traversible Python objects representing XML structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This installs the Python package manager pip. Then run the following:\n",
    "$pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running BeautifulSoup\n",
    "\n",
    "The most commonly used object in the BeautifulSoup library is, appropriately, the\n",
    "BeautifulSoup object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalit\\Miniconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\lalit\\Miniconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "bsObj = BeautifulSoup(html.read())\n",
    "print(bsObj.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the example before, we are importing the urlopen library and calling html.read() in order to get the HTML content of the page. This HTML content is then transformed into a BeautifulSoup object, with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html → <html><head>...</head><body>...</body></html>\n",
    "#head → <head><title>A Useful Page<title></head>\n",
    "#title → <title>A Useful Page</title>\n",
    "#body → <body><h1>An Int...</h1><div>Lorem ip...</div></body>\n",
    "#h1 → <h1>An Interesting Title</h1>\n",
    "#div → <div>Lorem Ipsum dolor...</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Reliably\n",
    "\n",
    "Let’s take a look at the first line of our scraper, after the import statements, and figureout how to handle any exceptions this might throw: html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "\n",
    "There are two main things that can go wrong in this line:\n",
    "• The page is not found on the server (or there was some error in retrieving it)\n",
    "• The server is not found\n",
    "\n",
    "In the first situation, an HTTP error will be returned. This HTTP error may be “404 Page Not Found,” “500 Internal Server Error,” etc. In all of these cases, the urlopen function will throw the generic exception “HTTPError”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can handle this exception in the following way:\n",
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "except HTTPError as e:\n",
    "     print(e)\n",
    " #return null, break, or do some other \"Plan B\"\n",
    "else:\n",
    " #program continues. Note: If you return or break in the\n",
    " #exception catch, you do not need to use the \"else\" statement\n",
    "    \n",
    "#If an HTTP error code is returned, the program now prints the error, and does not execute the rest of the program under the else statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the server is not found at all (if, say, http://www.pythonscraping.com was down, or the URL was mistyped), urlopen returns a None object.\n",
    "#We can add a check to see if the returned html is None:\n",
    " \n",
    "if html is None:\n",
    "    print(\"URL is not found\")\n",
    "else:\n",
    "    #program continues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if the page is retrieved successfully from the server, there is still the issue of the content on the page not quite being what we expected. Every time you access a tag in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually exists. If you attempt to access a tag that does not exist, BeautifulSoup will return a None object. The problem is, attempting to access a tag on a None object itself will result in an AttributeError being thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following line (where nonExistentTag is a made-up tag, not the name of a real BeautifulSoup function):\n",
    "print(bsObj.nonExistentTag)  # returns a None object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is perfectly reasonable to handle and check for. The trouble comes if you don’t check for it, but instead go on and try to call some other function on the None object, as illustrated in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bsObj.nonExistentTag.someTag)\n",
    "\n",
    "#which returns the exception:\n",
    "     #AttributeError: 'NoneType' object has no attribute 'someTag'\n",
    "    \n",
    "#So how can we guard against these two situations? The easiest way is to explicitly check for both situations:\n",
    "try:\n",
    "    badContent = bsObj.nonExistingTag.anotherTag\n",
    "except AttributeError as e:\n",
    "    print(\"Tag was not found\")\n",
    "else:\n",
    "    if badContent == None:\n",
    "         print (\"Tag was not found\")\n",
    "    else:\n",
    "         print(badContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code, for example, is our same scraper written in a slightly different way:\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "def getTitle(url):\n",
    "try:\n",
    "    html = urlopen(url)\n",
    "except HTTPError as e:\n",
    "    return None\n",
    "try:\n",
    "    bsObj = BeautifulSoup(html.read())\n",
    "    title = bsObj.body.h1\n",
    "     except AttributeError as e:\n",
    "             return None\n",
    " return title\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    " print(\"Title could not be found\")\n",
    "else:\n",
    " print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
